\chapter{Multicolor registration}
In microscopy it is often desirable to label different structures in a cell with
different colors. To do so our collaborators use different fluoroscent molecules
that emit light at different and distinguishable wavelengths. Using different
filters it is possible to capture pictures just containing light emited from one
fluorophore. To get a mulit-channel picture the different channels must be
aligned. Because different flourophores emit different wavelengths, chromatic
aberration apears. This means that the light for the same spot but with
different wavelenghts is not mapped to the same spot in the image. To align the
different channels despite chromatic aberration, beads are used. Beads are
flourophores added to the probe that emit light in all wavelengths the
different markers do, and therefore are visible in all channels. The beads can be
used as landmarks, because their position in the original image is at the same
spot. The task is to find a transformation that maps corresponding beads on each
other.

\section{Colorcomposer GUI}
The goal for the colorcomposer tool is to provide software that is easy to use, flexible and powerful. The current version of the colorcomposer is easy to use, because the different channels can be aligned by just selecting the auto align option.\newline
But it is also flexible. If the user wants to select the beads on his or her own, the beads can manually selected and deleted. After the transformation the user might use the implemented tools for colocalisation detection or save the transformed images and process them with the tool of his or her choice.\newline
Also the colorcomposer is powerful as it provides for example information about the number of points currently under the cursor or its intensity. Also the estimated transformation error and the localisation error are computed and stored.\newline
The basic framework for the colorcomposer was set up by \cite{MAJoachim}. It contained the workflow for importing and exporting images the handling of bead objects and a linear transformation that used the beads in the order they were found. This early version was not usable and were improved.\newline
Figure \ref{ColorComposer} shows improved colorcomposer GUI with two datasets loaded. The buttons on the right give the user the option to add or remove beads also in addition to the autodetected beads. There are also different sliders to control the values used for bead detection. In the lower right corner there is additional information provided about the total number of points within a rectengular with the selected cursor radius' size, also the sum of the intensities and the total number of frames for each data set is given. This information are helpful to determin whether or not a cluster of points is a bead or not.\newline
On top there is a menu bar with new options, like discarding all beads, automatically detecting beads, calculate colocalisation measures and show or hide the colocalisation heatmap.
\begin{figure}
\centering
\includegraphics[width = 0.88\textwidth]{pictures/GuiColorcomposer/BeforeAlignmentWithMenu.png}
\caption{Improved colorcomposer GUI. On the right sliders to set the parameters are present. There are also buttons to add or delete singel beads. On the lower right additional information about the area under the coursor is displayed, such as number of pixels, total intensity and the number of frames in total.}
\label{ColorComposer}
\end{figure}


\section{Features of the colorcomposer application}
\subsection{Invariance of input datas units}
The resulting coordinate files from SimpleSTORM or other STORM algorithms may be given in units of pixels relative to the unprocessed data or in nanometers. Treating coordinates given in nanometer as pixlel units would lead to very huge and very sparse images to display in the colorcomposer. Therefore the colorcomposer reads out additional information from the coordinates text files header. This informations are the pixel to nanometer ratio and the used factor. With this information the picture can be reconstructed as an upsampled version of the input image by the used factor regardless of the units used to save the coordinates file. If none of this information is given a pixel to nanometer ratio of 1 is assumed which garantues backward compatibility with older coordinate files given in pixel units.
\subsection{Manual bead selection and removal}
With the improved version of the colorcomposer it is possible to add beads manually. Therefore the desired location is clicked in the preview image and after that either the button "add green bead" or "add red bead" is hit. If there are enough frames containing localisations near the given location a bead is added to the center of mass of the intensities in that area. If the button "delete bead" is pressed all beads in the selected area are deleted.\newline
This feature can be used to add beads which the automatic bead detection missed.
\subsection{Automatic bead detection}
The input for the colorcomposer application is a text file created by the storm
algorithm that contains information about the position, intensity, symmetry,
frame number and signal-to-noise ratio of each detection. The beads should
ideally appear in most of the images. This means they can be found by searching for
detections that appear in almost every frame at the same position.\newline
There was already an automatic bead detection implemented by Joachim Schleicher. This was improved in the following ways.\newline
All important parameters for the bead detection can now be set in the GUI. The bead detection works by searching for points that appear in most of the frames. Instead of taking all localisations from the first frame as expected bead positions without considering locations that does not appear in the first frame, now a good subset of positions from the first 50 frames by skipping redundant positions based on the minimal distance of a new position to all positions already in the set. The range of 50 frames to look for beads is sufficiant because it is very unlikely that all 50 detections of the bead have been missed.\newline
After good candidates are found their number of points, variance and mean position is determined like described by \cite{MAJoachim}.\newline
In the end beads that are too close together are merged to for a new bead with its center right between the merged beads.

\subsection{Alignment of two multicolor images}
After the beads for each channel are found, the next task is to find the corresponding
beads in each channel. It can happen that some beads occur in just one channel.
If this is the case there will be no corresponding bead in the other
channels.\newline
To align the beads, the minimal number of beads, three, that are neccesary to
calculate the transformation are chosen randomly from the first channel. After that, based on
a probabilistic approach and a distance matrix containing information about the
distances between all beads of the two channels, three beads from the
second channel are chosen. It is more likely for nearer beads of the other channel to be selected, but any bead within a certain range can be chosen.\newline
Using these pairs of beads, a linear transformation is found, as described by
\cite{MAJoachim}.\newline
This transformation is used to tested how many other beads, that were not used to calculate the transformation match intotal. It is assumed that the correct transformation will match other bead pairs in addition.
This is very important because with every set of three points a valid transformation can be found that perfectly aligns this three beads. After that the whole
procedure is done multiple times. In the end the best transformation is
chosen based on the total number of bead pairs that match. If there are multiple transformations that match the same number of bead pairs, it is searched for the transformation with the lowest root mean square error for the matching beads.\\
In principle shearing should also be allowed for a linear transformation, but tests
indicate that shearing does not occur, so it is disabled to improve stability. If there are just
three beads in each channel, then every time a perfect transformation is found,
but with the constraint of forbidden shearing, the right solution can be
identified. There is an other problem with this transformation if the bead density is very high it might be that a transformation with much shearing is found that compresses the beads of one channel to a slim band. The probability to find a matching point by chance then is much greater then. Figure \ref{badshearing} shows the result of an incorrect transformation of simulated data. The red channel was created by randomly placing beads. The green channel was slightly shifted and rotated. The green channel was transformed to match with the red channel. Just a subset of beads were used to calculate the transformation and so this solution was found and chosen from the algorithm because of the additional matching point.\newline
This effects can be suppressed by not allowing shearing for the transformation.

\begin{figure}
\centering
\includegraphics[width = 0.88\textwidth]{pictures/shearingBad.png}
\caption{Affine transformation with shearing enabled. The green channel is deformed and one bead matches by chance which led to selection of this transformation.}
\label{badshearing}
\end{figure}
\subsection{Information about localisation certainty}
The detected spots from SimpleSTORM contain some localisation error. This error will be derived in section ref{detectionError}. It depends on the signal-to-noise ratio and the scale of the point spread function of a single fluorophore.

\section{Total localisation error}
There are four contributions to the total localisation error considered in this thesis.\newline
First there is the error introduced from the linear transformation to align the beads. To estimate this error the variance of the estimator is used. Since the transformation matrix is calculated by linear regression from the matricies $B$ and $R$ of the localisations of the first and the second layer of beads, with $B$ beeing the matrix of the first layer to that the second layer $R$ is transformed to. The variance of the estimator is
\begin{align}
\text{variance registration}&= \sigma^2 x_0\left(R^TR\right)^{-1}x_0^T, \text{ with }x_0 = \left(x_o,x_1,1\right) \label{gl22}
\end{align}
Using equation \ref{gl22} the variance for all pixel can be calculated.\newline
The second contribution to the total localisation error is the localisation error of the SimpleSTORM algorithm. Its formula is
\begin{align}
 \text{variance localisation} = \frac{N^2\pi}{2S_0^2} \left(1+\frac{\sigma_\text{PSF}^2}{\sigma_\text{filter}^2}\right)^2\left(\sigma_\text{filter}^2+\sigma_\text{PSF}^2\right)^2
\end{align}
It's derived in \ref{detectionError} and can be calculated for each detection individually based on the known signal-to-noise, which gives directly the signals intensity $S$ because of the known noise variance of one which gives the nois' standard deviation $N$ of also one. The PSFs width was either given or estimated and is passed to the colorcomposer in the detection coordinate file's header along with the prefactor $f$ for the actually used filter. Using this the localisation error can be writen as
\begin{align}
 \text{variance localisation} &= \frac{N^2\pi}{2S_0^2} \left(1+\frac{\sigma_\text{PSF}^2}{f^2\sigma_\text{PSF}^2}\right)^2\left(f^2\sigma_\text{PSF}^2+\sigma_\text{PSF}^2\right)^2 \\
 &=\frac{N^2\pi}{2S_0^2}\left(1+\frac{1}{f^2}\right)^2\left(1+f^2\right)^2\sigma_\text{PSF}^4\\
 &= \frac{N^2\pi}{2S_0^2}\left(2+\frac{1}{f^2}+f^2\right)^2\sigma_\text{PSF}^4
\end{align}
The third contribution results from the fact that the fluorophors are not directly attached to the sample of interest, but there are the antibodies used for staining inbetween. The upper bound of this error can be estimated under the assumption that the target of the antibodies is much smaller than the antibodies, so that there is no occlusion in the projection. This assumption does not hold in reality. Structures like cell membranes are certainly larger than the antibodies, but the assumption gives an upper bound. 
\begin{align}
\text{var x} &=\frac{1}{\Omega_\text{Sphere}}\int\limits_{\Omega_\text{Sphere}} x^2 dV\\ \frac{1}{\Omega_\text{Sphere}}\int\limits_0^{2\pi}\int\limits_0^\pi R^2\sin\theta \left(R\cos\phi \sin\theta\right)^2 d\theta d\phi\\
&=\frac{R^2}{4\pi} \int\limits_0^{2\pi}\int\limits_0^\pi \cos^2\phi \sin^3\theta d\theta d\phi\\
&=\frac{R^2}{4\pi} \left[\frac{1}{2}\phi +\frac{1}{4}\sin 2\phi \right]_0^{2\pi}\int\limits_0^\pi \sin^3\theta d\theta \\
&=\frac{ R^2}{2} \left[\frac{1}{12}\cos3\theta -\frac{3}{4}\cos\theta \right]_0^\pi\\
&=\frac{2}{3} R^2
\end{align}
$R$ is the assumed distance of the fluorophore to the structure it is attached to.\newline
The fourth and minor contribution to the total localisation error for each pixel is the quantization noise. It occurs when continous intensities, as the photons forming the PSF, are transformed into integer values. Quantisation noise describes the round-off error. The round-off error can be any value between -0.5 and 0.5 and is uniformly distributed. Its variance $\sigma_Q^2$ is
\begin{align}
 \sigma_Q^2 =\int\limits_{-\frac{1}{2}}^{\frac{1}{2}} x^2 dx = \frac{1}{12}
\end{align}
The unit of this error is the pixel size of the upscaled image and therefore more and more negligible the higher the upscaling factor is.\newline
To get the total localisation error all four variances are summed up for each pixel.

\section{Colocalisation}
Colocalisation in wide field microscopy is a measure of the overlap of data point from different channels. It can provide information whether or not two molecules interact. With increasing resolution of the images colocalisation becomes more and more a measure of similar structures near each other. Two structures can't be at the very same position in the cell. The colorcomposer software provides both global and local colocalisation measurements.
\subsection{Global colocalisation}
The most common colocalisation measure is Pearsons correlation coefficient \cite{pearson}. It is given as:
\begin{align}
\text{Pearson correlation coefficient =}\frac{\sum ^n _{i=1}(X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum ^n _{i=1}(X_i - \bar{X})^2} \sqrt{\sum ^n _{i=1}(Y_i - \bar{Y})^2}}
\end{align}
It is the ratio between the covariance between the points of two channels and their standard deviation.\newline

Also the Manders correlation coefficients $M_1$ and $M_2$ and the overlap coefficient (\cite{manders}) are calculated.
\begin{align}
M_1 =& \frac{\sum_i R_{i,\text{coloc}}}{\sum_i R_i}&M_2 = & \frac{\sum_i G_{i,\text{coloc}}}{\sum_i G_i}
\end{align}
With $R_{i,\text{coloc}} = R_i$ if $G_i >0$ and $R_{i,\text{coloc}} = 0$ otherwise and $G_{i,\text{coloc}} = G_i$ if $R_i >0$ and $G_{i,\text{coloc}} = 0$. $G_i$, $R_i$ are the intensities of the pixel of the green and red channel.
\begin{align}
\text{overlap coefficient} = \frac{\sum_i R_i \cdot G_i}{\sqrt{\sum_i \left(R_i\right)^2 \cdot \sum_i \left(G_i\right)^2}}
\end{align}

\subsection{Local colocalisation}
Global colocalisation has the drawback that there is just one value for the whole image. If there are regions in the image that show much colocalisation and other regions without colocalisation the same colocalisation coefficient might be achieved as if one channel is distributed randomly.\newline
For local colocalisation analysis the algorithm from \cite{coloc} are used and were further developed to gain a speed boost. The algorithm runs now approximatly 40 times faster. This was achieved by using scipys (\cite{scipy}) ckdtree function, which is a k-d tree implemented in C.
%\subsection{Validation of colocalisation approaches}
%"Image set CBS001RGM-CBS010RGM from the Colocalization Benchmark Source
%(www.colocalization-benchmark.com) was used to validate colocalization."