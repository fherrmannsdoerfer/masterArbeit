\chapter{Data processing}

\section{the data}
The datasets for STORM microscopy that we recieve from our collaborators from
Bioquant are big datasets of several gigabyte in the Andor .sif format. Each
file conains a stack of pictures, normaly between 1000 and 10000, taken
consecutively.
In each picture there are beads and signals. Both result from very small 
fluorescent molecules attached to the structures that are investigated. The
light of this pointlike objects is dissorted to a gaussian shaped signal due to
the large magnification.
Beads are molecules that emit light at any time contrary to the other signal
which blinks that means it is visible in just one frame at an explicit location.
The beads are used as landmarks for later alignment of two or more different
color channels. The other spots are the structure that the biologist are
interested in. Each of the gaussian shaped signals should be recognized and the
center will be determined with subpixel accuracy and is stored in the end in a
list to be further processed by the colorcomposer application.

\section{Parameters and options}
\subsection{Necessary}

\section{Import and processing}
The STORM data has usually a size of around 3 gigabyte. There are even larger datasets possible, so that it is important to work on smaller parts of the data, instead putting the whole dataset into memory. This is done using chunks of user defined size. The data is processed chunkwise, there is parallelisation for the frames of each chunk. This is possible because the signals in each frame are considered to be independent from each other.  

\section{Workflow}
\subsection{Chose parameters}
At the begining the user has the option to set all important parameters, if no parameter is set the default ones are used and will give a good result because all crucial parameters are either determined from the data or set to reasonable values that work for every data set.
\subsection{Estimating camera gain and offset}
First of all it is checked whether there exists a file containing settings for gain and offset from an earlier run. If this is not the case new parameters are estimated based on the first part of the data, usually 200 frames are sufficiant.
The method described by \cite{skellam} is used to estimate the
gain factor. For this methode a Skellam distribution is used. 
Each dataset is three-dimensional where time is the third
dimension. Therefore mean $\mu$ and variance $\sigma^2$ can be calculated from
the data for each pixel individually
\begin{align}
	\mu(i,j) & = \frac{\Sigma_t(I_t(i,j)-I_{t+1}(i,j))}{n}\\
	\sigma^2 & = \frac{\Sigma_t(\mu-(I_t(i,j)-I_{t+1}(i,j)))^2}{n-1}
\end{align} 
To determine the gain factor the Skellam parameter are plotted over the mean
intensities. A straight line can be fitted and its slope is exactly the gain
factor.
\subsection{Recursevly adjusting gain and offset}
After the estimation of gain factor and offset, the transformations described in \ref{trafoPoiss} and \ref{trafoAnscombe} are applied and the background subtracted.\newline
Due to the Anscombe transformation the background pixels of the image should only vary around a mean intensity of zero with a variance of 1. Therefore a histogramm of the pixel intensities is created. The after background subtraction the background pixels should contribute only to the lower intensities in the histogramm. A gaussian function is fitted to the histogramms values. This is done under the assumption that there is much more background in the image than signal or the intensities coming from signals are distributed over a larger range, so the gaussian for the background intensity distribution can be fitted correctly.\newline
If the estimated value for the variance is too far off 1 the originally estimated gain factor is corrected, applied and the fit is done again. This is done until the background variance converges or the maximal number of iterations is reached. In this case the initial gain factor will be used and a warning printed to the screen.
\subsection{Estimating the width of the point spread function}
For a certain number of frames the Fourier transform is calculated and averaged. The result is called mean power spectrum. It can be used to estimate the variance of the point spread function of the signal. A two dimensional gaussian functions Fourier transform is again a gaussian but with inverse variance. This relation is used to determine the variance of the point spread function in spatial domain, using the fit parameter for the variance in frequency domain.
\subsection{Processing the data}
\subsubsection{Import Data}
Storm data sets can consist of several thousand frames with resolutions up to one mega pixel per frame. This makes it necessary to break the data into smaller parts because otherwise it might be much larger as the RAM of an ordinary machine. Because of the background estimation it is not possible to process every frame completly independent as it was in the older version of this software \ref{joachim}. It is also faster with some datatypes to load a larger consecutive part of the dataset into memory.\newline
This algorithm uses chunks of user defined size. There are some limitations to the chuncksize that are discussed later. The data set is split into parts of equal size in $x$- and $y$-dimensions and independently also in $t$-dimension. If this partition does not fit at the edge of the data set, the last chunks will be smaller.\newline
The data is transformed to be Poission distributed and after that the Anscombe transform is applied.
\subsubsection{Background estimation}
For each chunk the median is determined to get a robust estimate of the background value for this chunk. 
The Bspline interpolation implemented in vigra \ref{vigra} is used to get interpolated values for the full resolution of the current frames. For this interpolation three chunks in $t$-dimension have to be available. Therefore the maximal chunksize in $t$-dimension must not be larger than a third of the total stacksize.\newline
This background is then subtracted from the transformed data to give finally background pixels with zero mean and a variance of one, both in $xy$- and in $t$-dimension.
\subsubsection{Create mask for background suppression}
With the given $p$-value from the settings a global threshold can be determined, because inhomogeneities of background intensities has been removed. The threshold value is that intensity that is explained from a gaussian distribution with mean zero and variance one with the given $p$ probability. This is possible because the background intensity follows such a distribution after all the applied transformations.\newline
The threshold is applied to the current frame and stored as a mask. Due to the probability that background pixels intensitis might exceed the threshold the connected components of the mask are calculated. Pixels that belong to connected components with too few members are discarded. 
\subsubsection{Filter data and finding maxima}
To imporve the accuracy of the spot detection the transformed signal is convolved with a two dimensional gaussian function with the previously determined or user set width. The convolved image will further be used to find the maxima. Each maxima found is tested to be covered by the mask or discared otherwise. A region of intrest around the remaining maxima is interpolated to a higher resolution. In the interpolated region  it is searched for maxima for the last time. This maxima will be detected with super resolution.\\
To determin the signal-noise-ratio the unfilterd and uninterpolated pixelintensity is used.

\subsubsection{Quality control for detections}
Especially in data sets with a high density of spots it can happen, that two spots are near each other and the point spread functions overlap. It may happen that instead of two maxima just one maximum will be detected right between the true ones. This leads to large errors in the localisation. To avoid this a threshold for the asymmetry of the spots can be set.




\section{Calibration measurement and plausibility}


\section{Accuracy of detection}
Unfortunately the position of the flourescent molecules can't be detected
perfectly. There are three main contribution to the error in detection.\\
First, there is the problem of finding the maximum in a noisy signal. Due to
noise the pixel next to the true maximum might get some intensity and be
therefore brighter.\\
Second, the choice of the gain factor and the offset might influence the
precision.\\
Third, the position is deteted by upscaling the pixel grid and interpolation.
After that the maximums position of the upscaled grid is taken as the resulting
position. This gives an error from roughly pixelwidth divided by square root of
two.
\subsection{error from noise}
Because there is no ground truth availible for micoscropy data, data must be
generated. This was done similar as described by \cite{simulated}.
\subsection{error from parameter estimation}

\section{Comparison with older version of the storm algorithm}
\section{Bleaching signal}

\section{Check for slope using calibration}
As described by \cite{meanVar} the true slope can be determined.

\section{New graphical user Interface}